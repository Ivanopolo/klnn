{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from kfkd import load2d\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading data...\")\n",
    "X, y = load2d()\n",
    "X_train = X[:1900]\n",
    "y_train = y[:1900]\n",
    "X_test = X[1900:]\n",
    "y_test = y[1900:]\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model and compiling functions...\n",
      "Starting training...\n",
      "Epoch 1 of 100 took 0.933s\n",
      "  training loss:\t\t0.059465\n",
      "  validation loss:\t\t0.007733\n",
      "Epoch 2 of 100 took 0.906s\n",
      "  training loss:\t\t0.014378\n",
      "  validation loss:\t\t0.005414\n",
      "Epoch 3 of 100 took 0.900s\n",
      "  training loss:\t\t0.009531\n",
      "  validation loss:\t\t0.005181\n",
      "Epoch 4 of 100 took 0.899s\n",
      "  training loss:\t\t0.007935\n",
      "  validation loss:\t\t0.005242\n",
      "Epoch 5 of 100 took 0.898s\n",
      "  training loss:\t\t0.007163\n",
      "  validation loss:\t\t0.005130\n",
      "Epoch 6 of 100 took 0.898s\n",
      "  training loss:\t\t0.006567\n",
      "  validation loss:\t\t0.004810\n",
      "Epoch 7 of 100 took 0.897s\n",
      "  training loss:\t\t0.006222\n",
      "  validation loss:\t\t0.004769\n",
      "Epoch 8 of 100 took 0.897s\n",
      "  training loss:\t\t0.005876\n",
      "  validation loss:\t\t0.004893\n",
      "Epoch 9 of 100 took 0.897s\n",
      "  training loss:\t\t0.005692\n",
      "  validation loss:\t\t0.004824\n",
      "Epoch 10 of 100 took 0.900s\n",
      "  training loss:\t\t0.005377\n",
      "  validation loss:\t\t0.004818\n",
      "Epoch 11 of 100 took 0.907s\n",
      "  training loss:\t\t0.005307\n",
      "  validation loss:\t\t0.004823\n",
      "Epoch 12 of 100 took 0.899s\n",
      "  training loss:\t\t0.005225\n",
      "  validation loss:\t\t0.004720\n",
      "Epoch 13 of 100 took 0.899s\n",
      "  training loss:\t\t0.005080\n",
      "  validation loss:\t\t0.004721\n",
      "Epoch 14 of 100 took 0.899s\n",
      "  training loss:\t\t0.005032\n",
      "  validation loss:\t\t0.004686\n",
      "Epoch 15 of 100 took 0.897s\n",
      "  training loss:\t\t0.005022\n",
      "  validation loss:\t\t0.004678\n",
      "Epoch 16 of 100 took 0.900s\n",
      "  training loss:\t\t0.004839\n",
      "  validation loss:\t\t0.004647\n",
      "Epoch 17 of 100 took 0.903s\n",
      "  training loss:\t\t0.004878\n",
      "  validation loss:\t\t0.004658\n",
      "Epoch 18 of 100 took 0.898s\n",
      "  training loss:\t\t0.004769\n",
      "  validation loss:\t\t0.004634\n",
      "Epoch 19 of 100 took 0.898s\n",
      "  training loss:\t\t0.004671\n",
      "  validation loss:\t\t0.004634\n",
      "Epoch 20 of 100 took 0.897s\n",
      "  training loss:\t\t0.004713\n",
      "  validation loss:\t\t0.004671\n",
      "Epoch 21 of 100 took 0.901s\n",
      "  training loss:\t\t0.004671\n",
      "  validation loss:\t\t0.004606\n",
      "Epoch 22 of 100 took 0.901s\n",
      "  training loss:\t\t0.004611\n",
      "  validation loss:\t\t0.004625\n",
      "Epoch 23 of 100 took 0.901s\n",
      "  training loss:\t\t0.004608\n",
      "  validation loss:\t\t0.004593\n",
      "Epoch 24 of 100 took 0.901s\n",
      "  training loss:\t\t0.004578\n",
      "  validation loss:\t\t0.004617\n",
      "Epoch 25 of 100 took 0.898s\n",
      "  training loss:\t\t0.004519\n",
      "  validation loss:\t\t0.004597\n",
      "Epoch 26 of 100 took 0.899s\n",
      "  training loss:\t\t0.004517\n",
      "  validation loss:\t\t0.004586\n",
      "Epoch 27 of 100 took 0.903s\n",
      "  training loss:\t\t0.004478\n",
      "  validation loss:\t\t0.004582\n",
      "Epoch 28 of 100 took 0.902s\n",
      "  training loss:\t\t0.004453\n",
      "  validation loss:\t\t0.004561\n",
      "Epoch 29 of 100 took 0.902s\n",
      "  training loss:\t\t0.004439\n",
      "  validation loss:\t\t0.004547\n",
      "Epoch 30 of 100 took 0.902s\n",
      "  training loss:\t\t0.004426\n",
      "  validation loss:\t\t0.004551\n",
      "Epoch 31 of 100 took 0.899s\n",
      "  training loss:\t\t0.004396\n",
      "  validation loss:\t\t0.004581\n",
      "Epoch 32 of 100 took 0.901s\n",
      "  training loss:\t\t0.004318\n",
      "  validation loss:\t\t0.004514\n",
      "Epoch 33 of 100 took 0.902s\n",
      "  training loss:\t\t0.004396\n",
      "  validation loss:\t\t0.004546\n",
      "Epoch 34 of 100 took 0.901s\n",
      "  training loss:\t\t0.004366\n",
      "  validation loss:\t\t0.004510\n",
      "Epoch 35 of 100 took 0.902s\n",
      "  training loss:\t\t0.004292\n",
      "  validation loss:\t\t0.004502\n",
      "Epoch 36 of 100 took 0.900s\n",
      "  training loss:\t\t0.004303\n",
      "  validation loss:\t\t0.004509\n",
      "Epoch 37 of 100 took 0.901s\n",
      "  training loss:\t\t0.004284\n",
      "  validation loss:\t\t0.004488\n",
      "Epoch 38 of 100 took 0.900s\n",
      "  training loss:\t\t0.004287\n",
      "  validation loss:\t\t0.004491\n",
      "Epoch 39 of 100 took 0.901s\n",
      "  training loss:\t\t0.004277\n",
      "  validation loss:\t\t0.004478\n",
      "Epoch 40 of 100 took 0.903s\n",
      "  training loss:\t\t0.004271\n",
      "  validation loss:\t\t0.004429\n",
      "Epoch 41 of 100 took 0.901s\n",
      "  training loss:\t\t0.004264\n",
      "  validation loss:\t\t0.004432\n",
      "Epoch 42 of 100 took 0.906s\n",
      "  training loss:\t\t0.004244\n",
      "  validation loss:\t\t0.004461\n",
      "Epoch 43 of 100 took 0.904s\n",
      "  training loss:\t\t0.004218\n",
      "  validation loss:\t\t0.004424\n",
      "Epoch 44 of 100 took 0.914s\n",
      "  training loss:\t\t0.004161\n",
      "  validation loss:\t\t0.004416\n",
      "Epoch 45 of 100 took 0.903s\n",
      "  training loss:\t\t0.004250\n",
      "  validation loss:\t\t0.004423\n",
      "Epoch 46 of 100 took 0.903s\n",
      "  training loss:\t\t0.004221\n",
      "  validation loss:\t\t0.004399\n",
      "Epoch 47 of 100 took 0.902s\n",
      "  training loss:\t\t0.004220\n",
      "  validation loss:\t\t0.004360\n",
      "Epoch 48 of 100 took 0.903s\n",
      "  training loss:\t\t0.004232\n",
      "  validation loss:\t\t0.004384\n",
      "Epoch 49 of 100 took 0.902s\n",
      "  training loss:\t\t0.004131\n",
      "  validation loss:\t\t0.004348\n",
      "Epoch 50 of 100 took 0.904s\n",
      "  training loss:\t\t0.004128\n",
      "  validation loss:\t\t0.004348\n",
      "Epoch 51 of 100 took 0.911s\n",
      "  training loss:\t\t0.004191\n",
      "  validation loss:\t\t0.004358\n",
      "Epoch 52 of 100 took 0.906s\n",
      "  training loss:\t\t0.004105\n",
      "  validation loss:\t\t0.004318\n",
      "Epoch 53 of 100 took 0.906s\n",
      "  training loss:\t\t0.004128\n",
      "  validation loss:\t\t0.004336\n",
      "Epoch 54 of 100 took 0.906s\n",
      "  training loss:\t\t0.004128\n",
      "  validation loss:\t\t0.004297\n",
      "Epoch 55 of 100 took 0.906s\n",
      "  training loss:\t\t0.004097\n",
      "  validation loss:\t\t0.004301\n",
      "Epoch 56 of 100 took 0.905s\n",
      "  training loss:\t\t0.004110\n",
      "  validation loss:\t\t0.004282\n",
      "Epoch 57 of 100 took 0.902s\n",
      "  training loss:\t\t0.004079\n",
      "  validation loss:\t\t0.004296\n",
      "Epoch 58 of 100 took 0.900s\n",
      "  training loss:\t\t0.004072\n",
      "  validation loss:\t\t0.004280\n",
      "Epoch 59 of 100 took 0.905s\n",
      "  training loss:\t\t0.004072\n",
      "  validation loss:\t\t0.004274\n",
      "Epoch 60 of 100 took 0.902s\n",
      "  training loss:\t\t0.004030\n",
      "  validation loss:\t\t0.004226\n",
      "Epoch 61 of 100 took 0.904s\n",
      "  training loss:\t\t0.004078\n",
      "  validation loss:\t\t0.004218\n",
      "Epoch 62 of 100 took 0.904s\n",
      "  training loss:\t\t0.004045\n",
      "  validation loss:\t\t0.004224\n",
      "Epoch 63 of 100 took 0.903s\n",
      "  training loss:\t\t0.004070\n",
      "  validation loss:\t\t0.004202\n",
      "Epoch 64 of 100 took 0.903s\n",
      "  training loss:\t\t0.003997\n",
      "  validation loss:\t\t0.004183\n",
      "Epoch 65 of 100 took 0.903s\n",
      "  training loss:\t\t0.004013\n",
      "  validation loss:\t\t0.004186\n",
      "Epoch 66 of 100 took 0.904s\n",
      "  training loss:\t\t0.004004\n",
      "  validation loss:\t\t0.004164\n",
      "Epoch 67 of 100 took 0.903s\n",
      "  training loss:\t\t0.003978\n",
      "  validation loss:\t\t0.004143\n",
      "Epoch 68 of 100 took 0.904s\n",
      "  training loss:\t\t0.003950\n",
      "  validation loss:\t\t0.004127\n",
      "Epoch 69 of 100 took 0.904s\n",
      "  training loss:\t\t0.003962\n",
      "  validation loss:\t\t0.004116\n",
      "Epoch 70 of 100 took 0.904s\n",
      "  training loss:\t\t0.003970\n",
      "  validation loss:\t\t0.004099\n",
      "Epoch 71 of 100 took 0.907s\n",
      "  training loss:\t\t0.003934\n",
      "  validation loss:\t\t0.004069\n",
      "Epoch 72 of 100 took 0.905s\n",
      "  training loss:\t\t0.003942\n",
      "  validation loss:\t\t0.004066\n",
      "Epoch 73 of 100 took 0.905s\n",
      "  training loss:\t\t0.003907\n",
      "  validation loss:\t\t0.004033\n",
      "Epoch 74 of 100 took 0.905s\n",
      "  training loss:\t\t0.003903\n",
      "  validation loss:\t\t0.004035\n",
      "Epoch 75 of 100 took 0.903s\n",
      "  training loss:\t\t0.003886\n",
      "  validation loss:\t\t0.004016\n",
      "Epoch 76 of 100 took 0.905s\n",
      "  training loss:\t\t0.003900\n",
      "  validation loss:\t\t0.003985\n",
      "Epoch 77 of 100 took 0.905s\n",
      "  training loss:\t\t0.003900\n",
      "  validation loss:\t\t0.003985\n",
      "Epoch 78 of 100 took 0.905s\n",
      "  training loss:\t\t0.003817\n",
      "  validation loss:\t\t0.003995\n",
      "Epoch 79 of 100 took 0.904s\n",
      "  training loss:\t\t0.003878\n",
      "  validation loss:\t\t0.003987\n",
      "Epoch 80 of 100 took 0.909s\n",
      "  training loss:\t\t0.003851\n",
      "  validation loss:\t\t0.003971\n",
      "Epoch 81 of 100 took 0.902s\n",
      "  training loss:\t\t0.003805\n",
      "  validation loss:\t\t0.003915\n",
      "Epoch 82 of 100 took 0.906s\n",
      "  training loss:\t\t0.003782\n",
      "  validation loss:\t\t0.003925\n",
      "Epoch 83 of 100 took 0.904s\n",
      "  training loss:\t\t0.003820\n",
      "  validation loss:\t\t0.003883\n",
      "Epoch 84 of 100 took 0.906s\n",
      "  training loss:\t\t0.003789\n",
      "  validation loss:\t\t0.003854\n",
      "Epoch 85 of 100 took 0.904s\n",
      "  training loss:\t\t0.003797\n",
      "  validation loss:\t\t0.003844\n",
      "Epoch 86 of 100 took 0.905s\n",
      "  training loss:\t\t0.003743\n",
      "  validation loss:\t\t0.003814\n",
      "Epoch 87 of 100 took 0.918s\n",
      "  training loss:\t\t0.003701\n",
      "  validation loss:\t\t0.003825\n",
      "Epoch 88 of 100 took 0.918s\n",
      "  training loss:\t\t0.003727\n",
      "  validation loss:\t\t0.003792\n",
      "Epoch 89 of 100 took 0.905s\n",
      "  training loss:\t\t0.003716\n",
      "  validation loss:\t\t0.003815\n",
      "Epoch 90 of 100 took 0.904s\n",
      "  training loss:\t\t0.003710\n",
      "  validation loss:\t\t0.003748\n",
      "Epoch 91 of 100 took 0.904s\n",
      "  training loss:\t\t0.003715\n",
      "  validation loss:\t\t0.003751\n",
      "Epoch 92 of 100 took 0.903s\n",
      "  training loss:\t\t0.003688\n",
      "  validation loss:\t\t0.003719\n",
      "Epoch 93 of 100 took 0.905s\n",
      "  training loss:\t\t0.003674\n",
      "  validation loss:\t\t0.003717\n",
      "Epoch 94 of 100 took 0.904s\n",
      "  training loss:\t\t0.003694\n",
      "  validation loss:\t\t0.003689\n",
      "Epoch 95 of 100 took 0.906s\n",
      "  training loss:\t\t0.003657\n",
      "  validation loss:\t\t0.003695\n",
      "Epoch 96 of 100 took 0.905s\n",
      "  training loss:\t\t0.003622\n",
      "  validation loss:\t\t0.003656\n",
      "Epoch 97 of 100 took 0.902s\n",
      "  training loss:\t\t0.003606\n",
      "  validation loss:\t\t0.003647\n",
      "Epoch 98 of 100 took 0.904s\n",
      "  training loss:\t\t0.003618\n",
      "  validation loss:\t\t0.003612\n",
      "Epoch 99 of 100 took 0.906s\n",
      "  training loss:\t\t0.003576\n",
      "  validation loss:\t\t0.003620\n",
      "Epoch 100 of 100 took 0.906s\n",
      "  training loss:\t\t0.003564\n",
      "  validation loss:\t\t0.003600\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne\n",
    "\n",
    "from kfkd import load2d, makeSubmit\n",
    "from utils import iterate_minibatches\n",
    "\n",
    "def build_cnn(input_var=None):\n",
    "\n",
    "    # Input layer:\n",
    "    network = lasagne.layers.InputLayer(shape=(None, 1, 96, 96),\n",
    "                                        input_var=input_var)\n",
    "    \n",
    "    # Convolutional layer with 32 kernels of size 5x5. Strided and padded\n",
    "    # convolutions are supported as well; see the docstring.\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=32, filter_size=(3, 3),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "\n",
    "    # Max-pooling layer of factor 2 in both dimensions:\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "\n",
    "    # Another convolution with 32 5x5 kernels, and another 2x2 pooling:\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=64, filter_size=(2, 2),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    \n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "    \n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=128, filter_size=(2, 2),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    \n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "\n",
    "    # A fully-connected layer of 256 units with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=0.5),\n",
    "            num_units=256,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "    # And, finally, the 10-unit output layer with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(network, p=0.5),\n",
    "            num_units=30,\n",
    "            nonlinearity=None)\n",
    "    \n",
    "    return network\n",
    "\n",
    "num_epochs=100\n",
    "\n",
    "# Prepare Theano variables for inputs and targets\n",
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.dmatrix('targets')\n",
    "\n",
    "# Create neural network model (depending on first command line parameter)\n",
    "print(\"Building model and compiling functions...\")\n",
    "network = build_cnn(input_var)\n",
    "\n",
    "# Create a loss expression for training, i.e., a scalar objective we want\n",
    "# to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.squared_error(prediction, target_var)\n",
    "loss = loss.mean()\n",
    "# We could add some weight decay as well here, see lasagne.regularization.\n",
    "\n",
    "# Create update expressions for training, i.e., how to modify the\n",
    "# parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "# Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(\n",
    "        loss, params, learning_rate=0.1, momentum=0.9)\n",
    "\n",
    "# Create a loss expression for validation/testing. The crucial difference\n",
    "# here is that we do a deterministic forward pass through the network,\n",
    "# disabling dropout layers.\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_loss = lasagne.objectives.squared_error(test_prediction, target_var)\n",
    "test_loss = test_loss.mean()\n",
    "\n",
    "# Compile a function performing a training step on a mini-batch (by giving\n",
    "# the updates dictionary) and returning the corresponding training loss:\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "\n",
    "# Compile a second function computing the validation loss and accuracy:\n",
    "val_fn = theano.function([input_var, target_var], [test_loss])\n",
    "\n",
    "# Compile function for prediction for test dataset\n",
    "test_fn = theano.function([input_var], [test_prediction])\n",
    "\n",
    "# Finally, launch the training loop.\n",
    "print(\"Starting training...\")\n",
    "# We iterate over epochs:\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train, 128, shuffle=True):\n",
    "        inputs, targets = batch\n",
    "        train_err += train_fn(inputs, targets)\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_err = val_fn(X_test, y_test)[0]\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load data for for submission\n",
    "X_submit, _ = load2d(test=True)\n",
    "\n",
    "#Make predictions with trained network\n",
    "preds = test_fn(X_submit)[0]\n",
    "\n",
    "#Create ./submission.csv that you should upload to Kaggle.com to get leaderboard results\n",
    "makeSubmit(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
